---
layout:     post
title:      IO详解
subtitle:   IO模型、select、epoll、poll
date:       2020-08-03
author:     deponia
header-img: img/post-sisyphus.jpg
catalog: true
tags:
    - IO
---



# 参考

https://zhuanlan.zhihu.com/p/115912936

https://www.jianshu.com/p/397449cadc9a

UNIX网络编程 书籍

# 1.IO模型

## 1.1 引入

![img](https://github.com/kennyfortune/kennyfortune.github.io/raw/master/img/v2-5311954c22d15ca91e47ab52168b7ada_720w.jpg)

重点在于应用层读入TCP/IP的时候

## 1.2 **阻塞式I/O模型**

**阻塞式I/O模型**：默认情况下，所有套接字都是阻塞的。怎么理解？先理解这么个流程，一个输入操作通常包括两个不同阶段：

（1）等待数据准备好；
（2）从内核向进程复制数据。


对于一个套接字上的输入操作，第一步通常涉及等待数据从网络中到达。当所有等待分组到达时，它被复制到内核中的某个缓冲区。第二步就是把数据从内核缓冲区复制到应用程序缓冲区。 好，下面我们以阻塞套接字的recvfrom的的调用图来说明阻塞

**优点**

- 简单、被阻塞时不占用CPU资源

**缺点**

- 全程阻塞、即使改用多线程也挺消耗资源

![202707564.jpg](https://github.com/kennyfortune/kennyfortune.github.io/raw/master/img/202707564.jpg)

## 1.3 **非阻塞式I/O**

**非阻塞式I/O**： 以下这句话很重要：进程把一个套接字设置成非阻塞是在通知内核，当所请求的I/O操作非得把本进程投入睡眠才能完成时，不要把进程投入睡眠，而是返回一个错误。看看非阻塞的套接字的recvfrom操作如何进行

**优先**

- 这种I/O方式也有明显的优势，即不会阻塞在内核的等待数据过程，每次发起的I/O请求可以立即返回，不用阻塞等待。在数据量收发不均，等待时间随机性极强的情况下比较常用。

**缺点**

- 轮询占用CPU

![202858660.jpg](https://github.com/kennyfortune/kennyfortune.github.io/raw/master/img/202858660.jpg)

## 1.4 I/O多路复用

**I/O多路复用**：虽然I/O多路复用的函数也是阻塞的，但是其与以上两种还是有不同的，I/O多路复用是阻塞在select，epoll这样的系统调用之上，而没有阻塞在真正的I/O系统调用如recvfrom之上。如图

**优点**

- 一个线程管理许多套接字，极大减少资源
- 虽然轮询但是是一个线程管理许多套接字，效率比非阻塞高很多

**缺点**

- 响应体过大时，排到很后面的io可能来不及响应、影响新时间轮询
- 在只处理连接数较小的场合，使用select的服务器不一定比多线程+阻塞I/O模型效率高，可能延迟更大，因为单个连接处理需要2次系统调用，占用时间会有增加。

![202009980.jpg](https://github.com/kennyfortune/kennyfortune.github.io/raw/master/img/202009980.jpg)

## 1.5 信号驱动I/O

![202028686.jpg](https://github.com/kennyfortune/kennyfortune.github.io/raw/master/img/202028686.jpg)

**优点**

- 很明显，我们的线程并没有在等待数据时被阻塞，可以提高资源的利用率

**缺点**

- 其实在Unix中，信号是一个被过度设计的机制(这句话来自知乎大神,有待考究)
  信号I/O在大量IO操作时可能会因为**信号队列溢出**导致没法通知——这个是一个非常严重的问题。
- 一般只适用于udp

## 1.6 异步I/O（需要底层OS的支持的）

类函数的工作机制是告知内核启动某个操作，并让内核在整个操作（包括将数据从内核拷贝到用户空间）完成后通知我们。如图：

![202055894.jpg](https://github.com/kennyfortune/kennyfortune.github.io/raw/master/img/202055894.jpg)

注意红线标记处说明在调用时就可以立马返回，等函数操作完成会通知我们。

## 1.7 以上五种的同步/非同步分类

![wKiom1LLqEPC2DSMAAUHeILYGZ4097.jpg](https://github.com/kennyfortune/kennyfortune.github.io/raw/master/img/wKiom1LLqEPC2DSMAAUHeILYGZ4097.jpg)

他们的区别在于第一阶段，而他们的第二阶段是一样的：在数据从内核复制到应用缓冲区期间（用户空间），进程阻塞于recvfrom调用。相反，异步I/O模型在这两个阶段都要处理。

再看POSIX对这两个术语的定义：

- 同步I/O操作：导致请求进程阻塞，直到I/O操作完成；
- 异步I/O操作：不导致请求进程阻塞。

好，下面我用我的语言来总结一下阻塞，非阻塞，同步，异步

- 阻塞，非阻塞：**进程/线程要访问的数据是否就绪，进程/线程是否需要等待；**
- 同步，异步：**访问数据的方式，同步需要主动读写数据，在读写数据的过程中还是会阻塞；异步只需要I/O操作完成的通知，并不主动读写数据，由操作系统内核完成数据的读写。**

# 2.多路复用的三种实现select、poll、epoll

## 2.1 select

![img](https://github.com/kennyfortune/kennyfortune.github.io/raw/master/img/20190527213148418.png)

**函数原型**

```c
int select(int maxfdp1,fd_set *readset,fd_set *writeset,fd_set *exceptset,const struct timeval *timeout);
```

**参数**

- **int maxfdp1** 指定待测试的文件描述字个数，它的值是待测试的最大描述字加1。
-  **fd_set \*readset , fd_set \*writeset , fd_set \*exceptset**
   `fd_set`可以理解为一个集合，这个集合中存放的是文件描述符(file descriptor)，即文件句柄。中间的三个参数指定我们要让内核测试读、写和异常条件的文件描述符集合。如果对某一个的条件不感兴趣，就可以把它设为空指针。
-  **const struct timeval \*timeout** `timeout`告知内核等待所指定文件描述符集合中的任何一个就绪可花多少时间。其timeval结构用于指定这段时间的秒数和微秒数。

**返回值**

- **int** 若有就绪描述符返回其数目，若超时则为0，若出错则为-1

> select()的机制中提供一种`fd_set`的数据结构，实际上是一个long类型的数组，每一个数组元素都能与一打开的文件句柄（不管是Socket句柄,还是其他文件或命名管道或设备句柄）建立联系，建立联系的工作由程序员完成，当调用select()时，由内核根据IO状态修改fd_set的内容，由此来通知执行了select()的进程哪一Socket或文件可读。
>
> 从流程上来看，使用select函数进行IO请求和同步阻塞模型没有太大的区别，甚至还多了添加监视socket，以及调用select函数的额外操作，效率更差。但是，使用select以后最大的优势是用户可以在一个线程内同时处理多个socket的IO请求。用户可以注册多个socket，然后不断地调用select读取被激活的socket，即可达到在同一个线程内同时处理多个IO请求的目的。而在同步阻塞模型中，必须通过多线程的方式才能达到这个目的

**优点**

- 多路复用的优点

**缺点**

- 每次调用select都需要重新设置一次fd_set

- 每次调用select，都需要把`fd_set`集合从用户态拷贝到内核态，如果`fd_set`集合很大时，那这个开销也很大

- 同时每次调用select都需要在内核遍历传递进来的所有`fd_set`，如果`fd_set`集合很大时，那这个开销也很大

- 为了减少数据拷贝带来的性能损坏，内核对被监控的`fd_set`集合大小做了限制，并且这个是通过宏控制的，大小不可改变(限制为1024)

- select返回后，又需要遍历所有的fd才能知道具体是哪个fd真正就绪了

  > ```csharp
  > void FD_CLR(int fd, fd_set *set);
  > int FD_ISSET(int fd, fd_set *set); //这个时判断哪个fd就绪了
  > void FD_SET(int fd, fd_set *set);
  > void FD_ZERO(fd_set *set)
  > ```

## 2.2 poll

**函数原型**

```c
int poll(struct pollfd *fds, nfds_t nfds, int timeout);

typedef struct pollfd {
        int fd;                         // 需要被检测或选择的文件描述符
        short events;                   // 对文件描述符fd上感兴趣的事件
        short revents;                  // 文件描述符fd上当前实际发生的事件
} pollfd_t;
```

**参数说明**

- **struct pollfd \*fds** `fds`是一个`struct pollfd`类型的数组，用于存放需要检测其状态的socket描述符，并且调用poll函数之后`fds`数组不会被清空；一个`pollfd`结构体表示一个被监视的文件描述符，通过传递`fds`指示 poll() 监视多个文件描述符。其中，结构体的`events`域是监视该文件描述符的事件掩码，由用户来设置这个域，结构体的`revents`域是文件描述符的操作结果事件掩码，内核在调用返回时设置这个域

- **nfds_t nfds** 记录数组`fds`中描述符的总数量

**返回值**

- **int** 函数返回fds集合中就绪的读、写，或出错的描述符数量，返回0表示超时，返回-1表示出错；

**优点**

- poll**没有最大文件描述符数量的限制**。

  > 其和select不同的地方：采用**链表**的方式替换原有fd_set数据结构,而使其**没有连接数的限制**。

**缺点**

- poll的机制与select类似，与select在本质上没有多大差别，管理多个描述符也是进行轮询，根据描述符的状态进行处理。也就是说，poll只解决了上面的文件描述符限制问题，其他都没有解决。

## 2.3 epoll

![img](https://github.com/kennyfortune/kennyfortune.github.io/raw/master/img/20190527231438974.png)

时间复杂度:O(1)

1. 调用epoll_create1创建一个epoll的fd
2. 多次调用epoll_ctl，将需要监听的fd与上一步中创建的epfd关联起来，同时将epoll_ctl的event参数的data.fd域设置为需要监听的fd
3. 循环调用epoll_wait
4. epoll_wait返回且返回值大于0，说明已经有fd准备就绪，根据返回值遍历epoll_wait的参数events，获取所有准备就绪的fd
5. 从fd上读取数据

**函数原型**

```c
int epoll_create1(int flags);
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
```

epoll_create1函数会创建一个epoll的fd并返回，在使用完epoll后必须将其关闭，否则会一直占用这个fd.

**参数**

epoll_ctl可以对传入的fd进行监听操作，各个参数含义如下：

- epfd：是epoll_create()的返回值。
- op：表示op操作，用三个宏来表示：添加EPOLL_CTL_ADD，删除EPOLL_CTL_DEL，修改EPOLL_CTL_MOD。分别添加、删除和修改对fd的监听事件。
- fd：是需要监听的fd（文件描述符）
- epoll_event：是告诉内核需要监听什么类型的事件，struct epoll_event结构如下：

```c
struct epoll_event {
__uint32_t events; /* Epoll events */
epoll_data_t data; /* User data variable */
};

//events可以是以下几个宏的集合，可以用 | 运算符组合多种事件：
EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；
EPOLLOUT：表示对应的文件描述符可以写；
EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；
EPOLLERR：表示对应的文件描述符发生错误；
EPOLLHUP：表示对应的文件描述符被挂断；
EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。
EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里

typedef union epoll_data {
　　void *ptr;
　　int fd;
　　uint32_t u32;
　　uint64_t u64;
} epoll_data_t;
```

**优点**

- 
  1. 对于需要监听的fd，只需要在初始化的时候调用一次epoll_ctl将fd与epfd相关联，后续就能循环调用epoll_wait监听事件了。无需像select一样，每次调用select方法的时候都要重复设置并传入待监听的fd集合。这样可以减少重复设置 fd_set、以及将fd_set在用户空间与kernel之间来回拷贝带来的开销
  
  2. epoll_wait方法返回的时候，可以直接从events参数中获取就绪的fd的信息，无需遍历整个fd集合。这样可以减少遍历fd_set带来的开销

     > 见上面得epoll_data_t里，里面有对应得fd和就绪信息，它存放再epoll_event里

  3. 在调用epoll_create1时，会在kernel中建立一颗fd红黑树与一个就绪fd链表，后续调用epoll_ctl中放入的fd会被挂载到这棵树上，同时也会在kernel的中断处理函数中注册一个回调函数。一旦某个正在监听的fd上有数据可读，kernel在把数据拷贝到内核缓存区中之后，还会将这个fd插入到就绪fd链表中。这样kernel就不用在有fd就绪的时候遍历整个fd集合，从而减少开销。
  
  
  

**水平触发与边缘触发**

- **Level_triggered(水平触发)：** 当被监控的文件描述符上有可读写事件发生时，epoll_wait()会通知处理程序去读写。如果这次没有把数据一次性全部读写完(如读写缓冲区太小)，那么下次调用 epoll_wait()时，它还会通知你在上没读写完的文件描述符上继续读写，当然如果你一直不去读写，它会一直通知你。如果系统中有大量你不需要读写的就绪文件描述符，而它们每次都会返回，这样会大大降低处理程序检索自己关心的就绪文件描述符的效率。

- **Edge_triggered(边缘触发)：**当被监控的文件描述符上有可读写事件发生时，epoll_wait()会通知处理程序去读写。如果这次没有把数据全部读写完(如读写缓冲区太小)，那么下次调用epoll_wait()时，它不会通知你，也就是它只会通知你一次，直到该文件描述符上出现第二次可读写事件才会通知你。这种模式比水平触发效率高，系统不会充斥大量你不关心的就绪文件描述符。

这两种触发的区别与epoll的实现机理有关。

在ET模式下，每次调用epoll_wait方法的时候，系统会直接将就绪链表清空，这样只有新就绪的fd才会被插入就绪链表并返回。

在LT模式下，每次调用epoll_wait方法时，系统只将就绪链表中的事件已经被处理完毕的fd（socket关联的kernel缓冲区数据已经被读取完毕）移除，如果某个fd上还有未被处理的数据，它会被保留在就绪链表中，并在epoll_wait返回时放在events参数中回送给用户。

```
ps. Java的nio就是用的水平触发。
```

# 3.高性能IO设计模式

## 3.1 经典设计

在传统的网络服务设计模式中，有两种比较经典的模式：

一种是多线程，一种是线程池。

**多线程**

对于多线程模式，也就说来了client，服务器就会新建一个线程来处理该client的读写事件，如下图所示：

![img](https://github.com/kennyfortune/kennyfortune.github.io/raw/master/img/aHR0cDovLzk1NjI5MDguczIxaS05LmZhaXVzci5jb20vMi9BQlVJQUJBQ0dBQWd6c09yd0FVb3R0V3E4UVV3d2dNNHFnTS5qcGc)

缺点明显、占用资源很大，线程的创建销毁。连接数量达到上限时，再有用户请求连接，直接会导致资源瓶颈，严重的可能会直接导致服务器崩溃。

**线程池**

考虑复用线程来改善

但是线程池也有它的弊端，如果连接大多是长连接，因此可能会导致在一段时间内，线程池中的线程都被占用，那么当再有用户请求连接时，由于没有可用的空闲线程来处理，就会导致客户端连接失败，从而影响用户体验。因此，线程池比较适合大量的短连接应用。

## 3.2 Reactor

在Reactor模式中，会先对每个client注册感兴趣的事件，然后有一个线程专门去轮询每个client是否有事件发生，当有事件发生时，便顺序处理每个事件，当所有事件处理完之后，便再转去继续轮询，如下图所示：

![img](https://github.com/kennyfortune/kennyfortune.github.io/raw/master/img/aHR0cDovLzk1NjI5MDguczIxaS05LmZhaXVzci5jb20vMi9BQlVJQUJBQ0dBQWc0TU9yd0FVb3Y3TE12QVV3M2dVNDZnTS5qcGc)

**也就是多路复用的设计**

## 3.3 Proactor

在Proactor模式中，当检测到有事件发生时，会新起一个异步操作，然后交由内核线程去处理，当内核线程完成IO操作之后，发送一个通知告知操作已完成。

**异步IO模型采用的就是Proactor模式。**